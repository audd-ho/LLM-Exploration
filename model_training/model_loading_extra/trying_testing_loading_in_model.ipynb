{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b578d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f8da88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PreTrainedModelClsPooling(torch.nn.Module):\n",
    "    def forward(self, input_embeddings):\n",
    "        pre_trained_last_hidden_state = input_embeddings[0]\n",
    "        return pre_trained_last_hidden_state[:, 0] ## cls embedding is first index for all batch data embedding\n",
    "\n",
    "class SentenceThemeTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, onehot = True):\n",
    "        self.data_df = df\n",
    "        self.data_sentences = self.data_df[\"sentences\"]\n",
    "        if not onehot:\n",
    "            ## integer output labeling, but same same? idk need test, but this integer isnt meant for ordinal anyway, the NN still one_hot-like, not single node output and all\n",
    "            self.data_labels = self.data_df[\"cat_label_index\"]\n",
    "        else:\n",
    "            ## can also try label with one hot encoding!! like a list or tensor of [0,0,1,0] then output is say [0.123,0.456,0.777,0.890], then back propagation and loss function to adjust to fit [0,0,1,0], maybe one hot encoding can work like that!!\n",
    "            self.data_labels = self.data_df[\"one_hot_labeling\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.len = len(self.data_df)\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data_sentences[index]\n",
    "        sentence_tokenized = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length = self.max_len,\n",
    "            ## pad_to_max_length = True ## depreciated so should not use\n",
    "            padding = \"max_length\", ## or True, longest, etc as padding value\n",
    "            return_attention_mask = True,\n",
    "            \n",
    "            truncation=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        sentence_label = self.data_labels[index]\n",
    "        \n",
    "        ## torch.tensor() along with dtype etc is likely needed but above got 'return_tensors=\"pt\"', so saved there, but output also(?), as in target and all? need check to ensure!! then fix accordingly!! also training using torch stuff so default all use torch data types and all!!, for consistency too!! \n",
    "        return {\n",
    "            \"input_ids\": sentence_tokenized[\"input_ids\"][0], ## the indexing since 'return_tensors=\"pt\"', then already have shape [1, 256]\n",
    "            \"attention_mask\": torch.tensor(sentence_tokenized[\"attention_mask\"].tolist()[0], dtype=torch.long), ## indexing and tolist(), is just to mimick what tokenizer returns if it isnt 'return_tensors=\"pt\"', then default is return list, and 1d(if only one string of text, not a list of them), so list will be 1d of len = 256, no batch size etc!!, and if list of text, then will 2d list, with list of lists(outputs in inner list), then torch.tensor() and dtype=torch.long is to convert the list to tensor and torch.long since tokenizer outputs are integers for attention mask (and input_ids)\n",
    "            \n",
    "            #\"data_label\": sentence_label ## need torch tensor it? or type need to be tensor? need to be torch also?, especially for output?, isit comparison for loss function etc need?\n",
    "            \"data_label\": torch.tensor(sentence_label, dtype=torch.float) ## torch.long not ideal since clasification means loss function is compare with decimal float stuff, so long vs float cannot so need consistent float both!!\n",
    "        } ## maybe data label give separate then see if can pass whole dict to output or drop data label from dict\n",
    "       \n",
    "\n",
    "class SentenceThemeTypeModel(torch.nn.Module):\n",
    "    def __init__(self, external_pretrained_model=None, max_len=0, num_labels=0, cls_only = False, torch_nn_sequential_kwargs_possible = False, load_mod=False):\n",
    "        super().__init__()\n",
    "        if load_mod:\n",
    "            return\n",
    "        if external_pretrained_model == None or max_len == 0 or num_labels == 0:\n",
    "            raise Exception(\"Something not fille!!\")\n",
    "            return\n",
    "        \n",
    "        self.cls_only = cls_only\n",
    "        self.torch_nn_sequential_kwargs_possible = torch_nn_sequential_kwargs_possible\n",
    "        \n",
    "        ## its torch.nn.Flatten(), to define an object instance of the torch.nn.Flatten class\n",
    "        ## not the torch.flatten(*params)\n",
    "        ## Flatten() object, then thus used, only flattens the dimension 1 to -1, thus keeping dimension 0, which is like the batch axis constant, unchanged\n",
    "        ## all above is default unless explicitly changed or modified params\n",
    "        self.flatten = torch.nn.Flatten() ## if need..., but text is 1d?, but at the same time, if tokens change then nodes change? as in number of nodes...\n",
    "        ## unless like disregard the number of tokens embedding, disregard the word embedding, just take one, like cls then fix so then always only 768 or whatever nodes isit?\n",
    "        ## unless cos padding so token size always fixed so can flatten? but idk if good or not??\n",
    "        ##!!\n",
    "        \n",
    "        self.pre_trained_model_cls_pooling = PreTrainedModelClsPooling()\n",
    "        \n",
    "        self.external_pretrained_model = external_pretrained_model\n",
    "        self.post_post_model_layer1 = torch.nn.Linear(768, 768)\n",
    "        self.post_post_model_layer2 = torch.nn.Linear(768, 768)\n",
    "        self.post_post_model_layer3 = torch.nn.Linear(768, num_labels)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        ## if kwargs_possible then can pass the kwargs into torch.nn.Sequential and pass to pretrained model, but if not then cannot\n",
    "        ## if kwargs cant pass into torch.nn.Sequential, then need to remove that layer and separate it first then inside the forward function!!\n",
    "        \n",
    "        if not self.cls_only:\n",
    "            if self.torch_nn_sequential_kwargs_possible:\n",
    "                ## flatten\n",
    "                self.flatten_post_model_layer = torch.nn.Linear((max_len*768), 768)\n",
    "                self.flatten_NN_stack_sequence = torch.nn.Sequential(\n",
    "                    self.external_pretrained_model,\n",
    "                    self.flatten,\n",
    "                    self.flatten_post_model_layer,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer1,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.dropout,\n",
    "                    self.post_post_model_layer2,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer3\n",
    "                )\n",
    "            else:\n",
    "                ## flatten\n",
    "                self.flatten_post_model_layer = torch.nn.Linear((max_len*768), 768)\n",
    "                self.flatten_NN_stack_sequence = torch.nn.Sequential(\n",
    "                    self.flatten,\n",
    "                    self.flatten_post_model_layer,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer1,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.dropout,\n",
    "                    self.post_post_model_layer2,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer3\n",
    "                )\n",
    "        else:\n",
    "            if self.torch_nn_sequential_kwargs_possible:\n",
    "                ## no flatten, cls only\n",
    "                self.cls_post_model_layer = torch.nn.Linear(768, 768)\n",
    "                self.cls_NN_stack_sequence = torch.nn.Sequential(\n",
    "                    self.external_pretrained_model,\n",
    "                    self.pre_trained_model_cls_pooling,\n",
    "                    self.cls_post_model_layer,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer1,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.dropout,\n",
    "                    self.post_post_model_layer2,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer3\n",
    "                )\n",
    "            else:\n",
    "                ## no flatten, cls only\n",
    "                self.cls_post_model_layer = torch.nn.Linear(768, 768)\n",
    "                self.cls_NN_stack_sequence = torch.nn.Sequential(\n",
    "                    self.pre_trained_model_cls_pooling,\n",
    "                    self.cls_post_model_layer,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer1,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.dropout,\n",
    "                    self.post_post_model_layer2,\n",
    "                    torch.nn.ReLU(),\n",
    "                    self.post_post_model_layer3\n",
    "                )\n",
    "    \n",
    "    def forward(self, **model_input_args): ## ** or *, and also how... cos ** need labeled right? hmm?, or drop the label when dataset out?\n",
    "        if not self.torch_nn_sequential_kwargs_possible:\n",
    "            # see if can just use sequence or if need do step by step for pretrained area\n",
    "            external_pretrained_model_output = self.external_pretrained_model(**model_input_args)\n",
    "            if not self.cls_only:\n",
    "                ## flatten\n",
    "                logits = self.flatten_NN_stack_sequence(external_pretrained_model_output)\n",
    "            else:\n",
    "                ## no flatten, cls only\n",
    "                logits = self.cls_NN_stack_sequence(external_pretrained_model_output)\n",
    "\n",
    "        else:\n",
    "            if not self.cls_only:\n",
    "                ## flatten\n",
    "                logits = self.flatten_NN_stack_sequence(**model_input_args)\n",
    "            else:\n",
    "                ## no flatten, cls only\n",
    "                logits = self.cls_NN_stack_sequence(**model_input_args)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a318e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distil_model = transformers.DistilBertModel.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "distil_tokeniser = transformers.DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35668788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, external_pretrained_model, max_len, num_labels, cls_only = False, torch_nn_sequential_kwargs_possible = False):\n",
    "\n",
    "the_model = SentenceThemeTypeModel(distil_model, 256, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bee8375",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './model_FF_trained/tokenizer_vocab.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    400\u001b[0m         path_or_repo_id,\n\u001b[0;32m    401\u001b[0m         filename,\n\u001b[0;32m    402\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    403\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    404\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    405\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    406\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    407\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    408\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    409\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    410\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    411\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './model_FF_trained/tokenizer_vocab.bin'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m the_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_FF_trained/pytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m## error since dk other config stuff about tokenizer, only know vocab, thats why distilbert load tokenizer, from_pretrained() can\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m the_tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_FF_trained/tokenizer_vocab.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:817\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 817\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    819\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:649\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    646\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    648\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 649\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    650\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    651\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    652\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    653\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    654\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    655\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    656\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    657\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    658\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    659\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    660\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    661\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    663\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    664\u001b[0m )\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:463\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: './model_FF_trained/tokenizer_vocab.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "the_model = torch.load(\"./model_FF_trained/pytorch_model.bin\")\n",
    "## error since dk other config stuff about tokenizer, only know vocab, thats why distilbert load tokenizer, from_pretrained() can\n",
    "the_tokenizer = transformers.AutoTokenizer.from_pretrained(\"./model_FF_trained/tokenizer_vocab.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "883ae977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IRASUser\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2004: FutureWarning: Calling DistilBertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "the_model = torch.load(\"./model_FF_trained/pytorch_model.bin\")\n",
    "the_tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"./model_FF_trained/tokenizer_vocab.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d24e0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save_vocabulary(), saves only the vocabulary file of the tokenizer (List of BPE tokens).\n",
    "## To save the entire tokenizer, you should use save_pretrained()\n",
    "# And should be an entire directory i guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d48b334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9864, -2.5324, -1.8298, -0.9164, -1.4597, -2.4646,  7.6528, -1.4592]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model(**the_tokenizer(\"Hi there!!\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54b30bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dict = {'positive': 0, 'negative': 1, 'neutral': 2, 'chaotic': 3, 'gibberish': 4, 'true_random_gibberish': 5, 'uwu': 6, 'leet': 7}\n",
    "new_dict = {v:k for k,v in old_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43b9ae2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true_random_gibberish'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict[torch.max(the_model(**the_tokenizer(\"\", return_tensors=\"pt\")), dim=1).indices.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2ea77a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.8399, -0.0347,  0.0516, -3.3037, -1.9892, -2.7872, -2.4891, -2.1345]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model(**the_tokenizer(\"Radiating warmth and kindness\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94990a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_2 = SentenceThemeTypeModel(distil_model, 2, 8)\n",
    "the_model_2 = torch.load(\"./model_FF_trained/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85433903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.8399, -0.0347,  0.0516, -3.3037, -1.9892, -2.7872, -2.4891, -2.1345]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model_2(**the_tokenizer(\"Radiating warmth and kindness\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4034e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_3 = SentenceThemeTypeModel(distil_model, 2, 8, cls_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1612ae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0449, -0.0159, -0.0039, -0.0230, -0.0023,  0.0185,  0.0290,  0.0188]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model_3(**the_tokenizer(\"Radiating warmth and kindness\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABOVE MODELS ARE BASED OFF ORIGINAL MODEL DEFINED CLASS AND INITIALISATION\n",
    "## below is added the kwargs so can just do \"load_mod=True\" so when need make a model instance, can just no need all those params, since going to be overwritten by loaded in model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5794bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_4 = SentenceThemeTypeModel(load_mod=True)\n",
    "the_model_4 = torch.load(\"./model_FF_trained/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f75da79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.8399, -0.0347,  0.0516, -3.3037, -1.9892, -2.7872, -2.4891, -2.1345]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model_4(**the_tokenizer(\"Radiating warmth and kindness\", return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6dcc38de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
