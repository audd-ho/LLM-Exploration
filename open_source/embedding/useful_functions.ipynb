{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "def get_length(embedding_1d):\n",
    "    sum = 0\n",
    "    for i in embedding_1d:\n",
    "        sum+=(i**2)\n",
    "    return math.sqrt(sum)\n",
    "def normalise_embedding(embedding_1d):\n",
    "    length = get_length(embedding_1d)\n",
    "    for i in range(len(embedding_1d)):\n",
    "        embedding_1d[i] /= length\n",
    "def get_normalise_embedding(embedding_1d):\n",
    "    if type(embedding_1d) is torch.Tensor:\n",
    "        temp_embedding_1d = (embedding_1d.detach().numpy()).copy()\n",
    "    else:\n",
    "        temp_embedding_1d = embedding_1d.copy()\n",
    "    length = get_length(temp_embedding_1d)\n",
    "    for i in range(len(temp_embedding_1d)):\n",
    "        temp_embedding_1d[i] /= length\n",
    "    return temp_embedding_1d\n",
    "\n",
    "\n",
    "def cosine_sim(embedding_1, embedding_2):\n",
    "    embedding_1 = get_normalise_embedding(embedding_1)\n",
    "    embedding_2 = get_normalise_embedding(embedding_2)\n",
    "    sim_sum = 0\n",
    "    for e_1, e_2 in zip(embedding_1, embedding_2):\n",
    "        sim_sum += (e_1*e_2)\n",
    "    return sim_sum\n",
    "def norm_ed_cosine_sim(embedding_1, embedding_2):\n",
    "    sim_sum = 0\n",
    "    for e_1, e_2 in zip(embedding_1, embedding_2):\n",
    "        sim_sum += (e_1*e_2)\n",
    "    return sim_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_sent_cos_sim(model_emb_func, t1, t2, additional_nesting = False):\n",
    "    if additional_nesting:\n",
    "        return cosine_sim(model_emb_func(t1)[0], model_emb_func(t2)[0])    \n",
    "    return cosine_sim(model_emb_func(t1), model_emb_func(t2))\n",
    "\n",
    "def test_emb_model(model_emb_func, sent_pair_comparison_list, sorting = False, additional_nesting = False):\n",
    "    ending_dict = {}\n",
    "    for comp1, comp2 in sent_pair_comparison_list:\n",
    "        ending_dict[(comp1, comp2)] = generic_sent_cos_sim(model_emb_func, comp1, comp2, additional_nesting)\n",
    "    if sorting:\n",
    "        sorted_ending_dict = {comps:comps_res for comps, comps_res in (sorted(ending_dict.items(), key=lambda dict_item: dict_item[1], reverse = True))}\n",
    "        return sorted_ending_dict\n",
    "    return ending_dict\n",
    "def test_emb_model_results(ending_dict, sorting = False):\n",
    "    print(\"Similarity level:\")\n",
    "    #res_sum = 0\n",
    "    if sorting:\n",
    "        for comps, res in (sorted(ending_dict.items(), key= lambda dict_item: dict_item[1], reverse=True)):\n",
    "            ## print(f\"{comps[0]:20.5}-{comps[1]:5.20}: {res:.5}\") # \"{:min_pad.max_pad}\", max pad is essentially also the max number of chars permitted!!\n",
    "            print(f\"{comps[0]:20.20} /-/ {comps[1]:20.20} : {res:.5}\")\n",
    "            #res_sum += res\n",
    "    else:\n",
    "        for comps, res in ending_dict.items():\n",
    "            ## print(f\"{comps[0]:20.5}-{comps[1]:5.20}: {res:.5}\") # \"{:min_pad.max_pad}\", max pad is essentially also the max number of chars permitted!!\n",
    "            print(f\"{comps[0]:20.20} /-/ {comps[1]:20.20} : {res:.5}\")\n",
    "            #res_sum += res\n",
    "        \n",
    "    ## res_sum no purpose and stuff yet since no measure of accuracy present, like it should/should not match, dont know so cannot say the sum is good or not, etc or avg, but later on can try with these and maybe weighted based on certain comparisons more impt?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg:\n",
    "## function is \"get_sentenceS_embedding\"\n",
    "## comparison list is \"comparison_pair_list\"\n",
    "res_dict = test_emb_model(get_sentenceS_embedding, comparison_pair_list, sorting=False , additional_nesting=True)\n",
    "test_emb_model_results(res_dict, sorting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_pair_list = [(\"tax relief\", \"employee benefit\"), (\"tax relief for employees\", \"employee benefit\"), (\"the world is green\", \"i hate soda\")]\n",
    "comparison_pair_list.append((\"yearly flight tickets home for employees\", \"home passage\"))\n",
    "comparison_pair_list.append((\"yearly flight tickets home for employees\", \"subsidised flights to employee home country\"))\n",
    "comparison_pair_list.append((\"night shift\", \"overtime pay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise tensor, but i think 1d tensor cannot? need to be inside another tensor to work?? idk, need read on docs or more maybe!\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generic useful for example: model is \"BAAI/bge-large-en-v1.5\"\n",
    "# model loading and eval?? are stuff to look at and maybe train further if can... but this one need look... idk how also... need read docs etc and look...\n",
    "# pooling and normalisation!!\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "#tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "#model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "#sentences = [\"样例数据-1\", \"样例数据-2\"]\n",
    "sentences = [\"i like you\", \"i want to kill you\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
